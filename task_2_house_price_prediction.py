# -*- coding: utf-8 -*-
"""Task:2_House Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tvi-enAmJU8NRTq_PIO8rJRRILG1e6hb
"""

#Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import warnings
warnings.filterwarnings('ignore')

#Loading data
data = pd.read_csv('Bengaluru_House_Data.csv')

#Checking shape of the data
data.shape

#Checking top five data
data.head()

#Checking data information
data.info()

#Extracting value count of categorical variables
cat_var = ['area_type', 'availability', 'location', 'size', 'society', 'total_sqft']

for i in cat_var:
    print(i,':=>',len(set(data[i])))
    print(data[i].value_counts())
    print('O~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~O')

#Creating correlation matrix
plt.figure(figsize = (13,6))
# Select only numeric columns before calculating correlation
numeric_data = data.select_dtypes(include=np.number)
sns.heatmap(numeric_data.corr(), annot = True);

#Checking missing values in the data
missing_values = data.isnull().sum()[data.isnull().sum()>0]
missing_values.sort_values(ascending = False)

#Checking missing values in the data (Proportional form)
missing_values = data.isnull().sum()[data.isnull().sum()>0] / data.shape[0] * 100
missing_values.sort_values(ascending = False).round(2)

#Visualizing missing values
plt.figure(figsize = (16,9))
sns.heatmap(data.isnull());

#Dropping society variable as it has 41.31% missing values
data2 = data.drop(columns='society', axis = 1)

#Missing value imputation by mean in balcony variable
data2['balcony'] = data2['balcony'].fillna(data2['balcony'].mean())

#Checking missing values in the data
missing_values = data2.isnull().sum()[data2.isnull().sum()>0]
missing_values.sort_values(ascending = False)

#Missing value imputation by mode in bath variable
data2['bath'] = data2['bath'].fillna(data2['balcony'].mode()[0])

#Checking missing values in the data
missing_values = data2.isnull().sum()[data2.isnull().sum()>0]
missing_values.sort_values(ascending = False)

#Dropping rest 17 rows from the data
data3 = data2.dropna()

#Creating copy of data3
data4 = data3.copy()

#Creating list of different measures storing converted values of square feet
hyphen = []
yards = []
acres = []
meter = []
perch = []
cents = []
guntha = []
ground = []

#Converting values into average square feet have a range of square feet
for g in data4['total_sqft'][data4['total_sqft'].str.contains('-')]:
  splited_val = g.split('-')
  val_hyphen = float(splited_val[0]) + float(splited_val[1])
  val_hyphen_sqft = val_hyphen/2
  hyphen.append(val_hyphen_sqft)

#Converting values of yard into square feet
for h in data4['total_sqft'][data4['total_sqft'].str.contains('Yards')]:
  splited_val2 = h.split('S')
  val_yards = float(splited_val2[0])
  val_yard_sqft = val_yards*9
  yards.append(val_yard_sqft)

#Converting values of acres into square feet
for i in data4['total_sqft'][data4['total_sqft'].str.contains('Acres')]:
  splited_val3 = i.split('A')
  val_acres = float(splited_val3[0])
  val_acres_sqft = val_acres*43560
  acres.append(val_acres_sqft)

#Converting values of perch into square feet
for j in data4['total_sqft'][data4['total_sqft'].str.contains('Perch')]:
  splited_val4 = j.split('P')
  val_perch = float(splited_val4[0])
  val_perch_sqft = val_perch*272.25
  perch.append(val_perch_sqft)

#Converting values of meter into square feet
for k in data4['total_sqft'][data4['total_sqft'].str.contains('Sq. Meter')]:
  splited_val1 = k.split('S')
  val_meter = float(splited_val1[0])
  val_meter_sqft = val_meter*10.7639
  meter.append(val_meter_sqft)

#Converting values of cents into square feet
for m in data4['total_sqft'][data4['total_sqft'].str.contains('Cents')]:
  splited_val5 = m.split('Ce')
  val_cents = float(splited_val5[0])
  val_cents_sqft = val_cents*435.540069686
  cents.append(val_cents_sqft)

#Converting values of guntha into square feet
for n in data4['total_sqft'][data4['total_sqft'].str.contains('Guntha')]:
  splited_val6 = n.split('Gu')
  val_guntha = float(splited_val6[0])
  val_guntha_sqft = val_guntha*1089.08734481
  guntha.append(val_guntha_sqft)

#Converting values of grounds into square feet
for o in data4['total_sqft'][data4['total_sqft'].str.contains('Grounds')]:
  splited_val7 = o.split('Gr')
  val_ground = float(splited_val7[0])
  val_ground_sqft = val_ground*2400
  ground.append(val_ground_sqft)

#Extracting and values have integer and float and converting it into integer
total_sqft_int = data4['total_sqft'][~data4['total_sqft'].str.contains('-') & ~data4['total_sqft'].str.contains('Yards') & ~data4['total_sqft'].str.contains('Acres') & ~data4['total_sqft'].str.contains('Meter') & ~data4['total_sqft'].str.contains('Perch') & ~data4['total_sqft'].str.contains('Cents') & ~data4['total_sqft'].str.contains('Guntha') & ~data4['total_sqft'].str.contains('Grounds')]
total_sqft_int = total_sqft_int.map(float)

total_sqft_df = pd.concat([pd.DataFrame(yards, data4['total_sqft'][data4['total_sqft'].str.contains('Yards')].index),
           pd.DataFrame(acres, data4['total_sqft'][data4['total_sqft'].str.contains('Acres')].index),
           pd.DataFrame(meter, data4['total_sqft'][data4['total_sqft'].str.contains('Sq. Meter')].index),
           pd.DataFrame(perch, data4['total_sqft'][data4['total_sqft'].str.contains('Perch')].index),
           pd.DataFrame(cents, data4['total_sqft'][data4['total_sqft'].str.contains('Cents')].index),
           pd.DataFrame(guntha, data4['total_sqft'][data4['total_sqft'].str.contains('Guntha')].index),
           pd.DataFrame(ground, data4['total_sqft'][data4['total_sqft'].str.contains('Grounds')].index),
           pd.DataFrame(hyphen, data4['total_sqft'][data4['total_sqft'].str.contains('-')].index),
           total_sqft_int],
          axis = 0)
total_sqft_df = total_sqft_df.sort_index(axis=0)

#Checking total_sqft_df
total_sqft_df

#Adding cleaned variable total_sqft
data4['total_sqft_new'] = total_sqft_df['total_sqft']

#Checking value counts of size variable
data4['size'].value_counts()

#Extracting integer from the size
size = []
for i in data4['size']:
  size.append(i.split(' ')[0])

#Creating new variable size new from size list
data4['size_new'] = size

#Changing the data type of size_new to integer
data4['size_new'] = data4['size_new'].astype(int)

#Dropping unwanted variables size & total_sqft
data4 = data4.drop(columns = ['total_sqft', 'size'])

#Creating data copy of data4
data5 = data4.copy()

#Creating diagnostic plot on the data5
def DIAGNOSTIC_PLOTS(data5, variable):
  plt.figure(figsize=(16,4))
  plt.subplot(1,3,1)
  sns.distplot(data5[variable])
  plt.title('Distribution Plot')
  plt.subplot(1,3,2)
  sns.boxplot(data5[variable])
  plt.title('Box Plot')
  plt.subplot(1,3,3)
  stats.probplot(data5[variable], dist = 'norm', plot = plt)
  plt.show()

var = ['bath', 'balcony', 'total_sqft_new', 'size_new','price']
for i in var:
  print(i)
  DIAGNOSTIC_PLOTS(data5,i)

#Creating regression plot
var = ['bath', 'balcony', 'total_sqft_new', 'size_new']

plt.figure(figsize=(16,9))
for i,j in enumerate(var):
    plt.subplot(2,2,i+1)
    sns.regplot(x = data5[j], y=data5['price'], line_kws={'color':'red'});
plt.subplots_adjust(hspace=0.5)

#Extracting the data where a average sqft is less than 350
data5[data5['total_sqft_new']/data5['size_new']<350].head()

#Removing the data where a average sqft is less than 350
data6 = data5[~(data5['total_sqft_new']/data5['size_new']<350)]
data6.head()

#Creating new variable price per square feet
data6['price_per_sqft'] = data6['price'] / data6['total_sqft_new'] * 100000

#Checking data stats summary of price per square feet
data6['price_per_sqft'].describe()

#Removing outliers from price per sqft
mean = np.mean(data6['price_per_sqft'])
std = np.std(data6['price_per_sqft'])
data7 = data6[(data6['price_per_sqft']>(mean-std)) & (data6['price_per_sqft']<=(mean+std))]

#Resetting index number
data7.reset_index(drop=True)

#Creating diagnostic plot on the data7
def DIAGNOSTIC_PLOTS(data7, variable):
  plt.figure(figsize=(16,4))
  plt.subplot(1,3,1)
  sns.distplot(data7[variable])
  plt.title('Distribution Plot')
  plt.subplot(1,3,2)
  sns.boxplot(data7[variable])
  plt.title('Box Plot')
  plt.subplot(1,3,3)
  stats.probplot(data7[variable], dist = 'norm', plot = plt)
  plt.show()

var = ['bath', 'balcony', 'total_sqft_new', 'size_new','price', 'price_per_sqft']
for i in var:
  DIAGNOSTIC_PLOTS(data7,i)

#Extracting the data where bath value greater than size of flat+2
data7[data7['bath']>data7['size_new']+2]

#Extracting the data where bath value less than size of flat + 2
data8 = data7[data7['bath']<data7['size_new']+2]

#Creating diagnostic plot on the data8
def DIAGNOSTIC_PLOTS(data8, variable):
  plt.figure(figsize=(16,4))
  plt.subplot(1,3,1)
  sns.distplot(data8[variable])
  plt.title('Distribution Plot')
  plt.subplot(1,3,2)
  sns.boxplot(data8[variable])
  plt.title('Box Plot')
  plt.subplot(1,3,3)
  stats.probplot(data8[variable], dist = 'norm', plot = plt)
  plt.show()

var = ['bath', 'balcony', 'total_sqft_new', 'size_new','price', 'price_per_sqft']
for i in var:
  DIAGNOSTIC_PLOTS(data8,i)

#Importing library for label coding
from sklearn.preprocessing import LabelEncoder

#Implementing label encoding on area type, availability & location variable
le = LabelEncoder()
data8['area_type'] = le.fit_transform(data8['area_type'])
data8['availability'] = le.fit_transform(data8['availability'])
data8['location'] = le.fit_transform(data8['location'])

#Checking data
data8.head()

#Splitting data as X & y
X = data8.drop(columns = 'price')
y = data8['price']

#Importing library for train and test split
from sklearn.model_selection import train_test_split

#Splitting data into train(80%) & test(20%)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)

#Importing regressor from linear model
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import HuberRegressor
from sklearn.linear_model import ElasticNetCV

#Importing regressor from ensemble
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import GradientBoostingRegressor

#Importing regressor from others
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

#Importing libraries for rmse, cross validation & parameter tuning
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV

#Creating function which train a model and extract accuracy, rmse & cv score
def TRAIN_MODEL(model, x_train, x_test, y_train, y_test):
  model.fit(x_train, y_train)
  y_pred = model.predict(x_test)
  accuracy = model.score(x_test, y_test)
  rmse = np.sqrt(mean_squared_error(y_test, y_pred))
  cv = cross_val_score(model, x_train, y_train, cv=5)
  cv_score = cv.mean()
  return {'Accuracy':accuracy,'RMSE': rmse, 'CV Score': cv_score}

#Train a model and extract accuracy, rmse & cv score from all the regressors
lr = LinearRegression()
ls = Lasso()
rd = Ridge()
hr = HuberRegressor()
en = ElasticNetCV()
#------------------------------------
rf = RandomForestRegressor()
etr = ExtraTreesRegressor()
gbr = GradientBoostingRegressor()
#------------------------------------
svr = SVR()
dt = DecisionTreeRegressor()
xgb = XGBRegressor(verbosity=0)

model = [lr, ls, rd, hr, en, rf, etr, gbr, svr, dt, xgb]
model_name = ['Linear Regression', 'Lasso Regression', 'Ridge Regression', 'Huber Regression', 'ElasticNet CV' ,
         'Random Forest Regressor', 'Extra Tree Regressor', 'Gradient Boosting Regressor', 'Support Vector Regressor',
         'Decision Tree Regressor', 'XGB Regressor']
for i,j in zip(model, model_name):
  print(j,'=>',TRAIN_MODEL(i,X_train, X_test, y_train, y_test))

#Train a model with best parameters
xgb_tune = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=6,
             min_child_weight=1, missing=np.nan, monotone_constraints='()',
             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None)

xgb_tune.fit(X_train, y_train)
y_pred_tune = xgb_tune.predict(X_test)
accuracy = xgb_tune.score(X_test, y_test)
RMSE = np.sqrt(mean_squared_error(y_test,y_pred_tune))
cv = cross_val_score(xgb_tune, X_train, y_train, cv=5)
cv_score = cv.mean()
print('Accuracy',accuracy, 'RMSE',RMSE, 'CV Score',cv_score)

#Manual parameter tuning & train a model
xgb_tune2 = XGBRegressor(base_score = 0.5,
                        booster = 'gbtree',
                        colsample_bylevel = 1,
                        importance_type = 'gain',
                        learning_rate = 0.25,
                        max_delta_step = 0,
                        max_depth = 4,
                        min_child_weight = 1,
                        missing = np.nan,
                        n_estimators = 400,
                        n_jobs = 1,
                        nthread = None,
                        objective = 'reg:linear',
                        random_state = 0,
                        reg_alpha = 0,
                        reg_lambda = 1,
                        scale_pos_weight =1,
                        seed = None,
                        silent = None,
                        subsample = 1,
                        verbosity = 1)
xgb_tune2.fit(X_train, y_train)
xgb_tune2.score(X_test, y_test)

#Checking cross validation on manual parameter model
cvs = cross_val_score(xgb_tune2, X_train, y_train, cv=5)
cvs, cvs.mean()

#Checking RMSE on manual parameter model
np.sqrt(mean_squared_error(y_test, xgb_tune2.predict(X_test)))